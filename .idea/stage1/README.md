# ğŸ§  Search Engine Big Data â€“ Stage 1

This project implements the **Stage 1 data pipeline** for a simple search engine to search through Gutenberg.org.  
It includes:
- Two data crawlers (JSON and TXT),
- Two indexers with inverted indexing,
- Two databanks as metadata repositories
- A query engine for term-based search for every indexer,
- A control panel that orchestrates the pipeline,
- Benchmarking for performance analysis.

---

## ğŸ“¦ Project Overview

```

## ğŸ“¦ Project Overview


stage1/
â”‚
â”œâ”€â”€ crawlers/
â”‚   â”œâ”€â”€ crawler_v1.py          # Crawler (JSON format)
â”‚   â””â”€â”€ crawler_v2.py          # Crawler (TXT format, 3-part files)
â”‚
â”œâ”€â”€ indexers/
â”‚   â””â”€â”€ indexer_v1.py          # Indexer with tokenization and inverted index
â”‚
â”œâ”€â”€ indexer_query/
â”‚   â””â”€â”€ query_engine.py        # Single and multi-term search
â”‚
â”œâ”€â”€ control/
â”‚   â””â”€â”€ control_panel.py       # Manages downloading + indexing workflow
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ cleanup_project.py     # Deletes all datalakes and control files
â”‚   
â”‚
â”œâ”€â”€ benchmarking/
â”‚   â”œâ”€â”€ benchmark_index_scaling.py # Index scaling + search benchmark
â”‚   â”œâ”€â”€ benchmark_indexer_v1.py    # Indexer benchmarking
â”‚
â”œâ”€â”€ data_repository/
â”‚   â”œâ”€â”€ datalake_v1/          # JSON files (crawler v1)
â”‚   â”œâ”€â”€ datalake_v2/          # TXT files (crawler v2)
â”‚   â”œâ”€â”€ datamart_indexer_v1/  # Indexed inverted index JSON
â”‚
â”œâ”€â”€ Main.py                   # Optional entry point
â””â”€â”€ README.md

'''

 ğŸ§° Requirements

### ğŸ Python
**Python 3.10 or later** is required (tested with Python 3.13).  

### ğŸ“¦ Install dependencies
Install all required packages using pip:

```bash
pip install requests nltk memory-profiler psutil sortedcontainers
````

If you use NLTK for stopwords, run this once:

```python
import nltk
nltk.download("stopwords")
```

---

## ğŸš€ Setup Instructions

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/<your-username>/SearchEngineBigData.git
cd SearchEngineBigData
```

### 2ï¸âƒ£ Mark stage1 as a Python package root

Ensure that every folder contains an `__init__.py` file.

### 3ï¸âƒ£ Run the functions

Run Scripts in the ``Main.py`` file

---

## ğŸ§© Components and Usage

### ğŸ•¸ï¸ Crawler v1 (JSON-based)

Downloads Project Gutenberg texts and stores each as a JSON file
with three sections: header, content, footer.

Run manually:

```python
from stage1.crawlers.crawler_v1 import download_book_v1
download_book_v1(76921)
```

ğŸ“ **Output:**

```
data_repository/datalake_v1/76921.json
```

---

### ğŸ“‚ Crawler v2 (TXT-based)

Downloads and splits books into three text files:
`<id>_header.txt`, `<id>_content.txt`, `<id>_footer.txt`.

Run manually:

```python
from stage1.crawlers.crawler_v2 import download_book_v2
download_book_v2(76921)
```

ğŸ“ **Output:**

```
data_repository/datalake_v2/76001-77000/76921_content.txt
```

---

### âš™ï¸ Indexer v1

Builds an inverted index from the JSON datalake (v1).
Handles tokenization, normalization, and stopword removal.

Run manually:

```python
from stage1.indexers.indexer_v1 import build_inverted_index, save_index
index = build_inverted_index()
save_index(index)
```

ğŸ“ **Output:**

```
data_repository/datamart_indexer_v1/inverted_index.json
```

---

### ğŸ” Indexer_v1 Query Engine

Allows single-term and multi-term searches over the index.

Example:

```python
from stage1.query.query_engine import search, search_multiple

search("house")  
# â†’ ['70001', '70023']

search_multiple(["white", "house"], mode="and")  
# â†’ ['70023']
```

---

### ğŸ§  Control Panel

Coordinates the data pipeline:

* Downloads books
* Indexes them
* Tracks progress

Run:

```bash
python -m stage1.control.control_panel
```

Or from code:

```python
from stage1.control.control_panel import control_pipeline_step
control_pipeline_step()
```

âœ… Automatically checks which books are downloaded/indexed
and triggers the next step accordingly.

---

## ğŸ§ª Benchmarking

### 1ï¸âƒ£ Index Scaling + Index search v1 Benchmark

Measures how indexing performance scales with dataset size and how query time behaves.

Run:

```bash
python -m stage1.benchmarking.benchmark_indexer_v1
```

ğŸ“Š **Output:**

```
data_repository/benchmark_index_scaling.csv
```

**Columns:**
| Number of Books | Indexing Time (s) | Memory (MB) | File Size (MB) | Search (advantage) [Âµs] | Search (house) [Âµs] | Search (white) [Âµs] |

---

### 2ï¸âƒ£ Other Benchmarks

* `benchmark_indexer_v1.py` â†’ Tests index build performance.
* `benchmark_crawlers.py` â†’ Compares crawler v1 and v2.

All can be run similarly:

```bash
python -m stage1.benchmarking.<filename_without_py>
```

---

## ğŸ§¹ Cleanup Utilities

### Delete everything (reset project)

```python
from stage1.utils.cleanup_project import cleanup_project
cleanup_project(confirm=False)
```

Deletes:

* `data_repository/datalake_v1`
* `data_repository/datalake_v2`
* `data_repository/datamart_indexer_v1`
* Control files (`downloaded_books.txt`, `indexed_books.txt`)

---

### Delete only the indexed file

```python
from stage1.utils.project_cleanup import delete_indexed_file
delete_indexed_file()
```

---

## ğŸ“ˆ Typical Workflow

### 1ï¸âƒ£ Start fresh

```python
from stage1.utils.cleanup_project import cleanup_project
cleanup_project(confirm=False)
```

### 2ï¸âƒ£ Run Control Panel (downloads + indexes automatically)

```bash
python -m stage1.control.control_panel
```

### 3ï¸âƒ£ Search your index

```python
from stage1.indexer_query.indexed_query_v1 import search_file_v1
print(search_file_v1("advantage"))
```



---

## ğŸ§‘â€ğŸ’» Authors

Project developed as part of **Big Data â€“ Stage 1 (2025)**
**Search Engine Big Data Group @ University Project** by Marco Keilwagen, Kacper Clasen, Richard Raatz, Jonas MÃ¼ller and Milosch FÃ¼rsteneberg

---

## ğŸ“œ License

This project is for **academic use**.
All downloaded Project Gutenberg texts are **public domain**,
but please respect [Project Gutenbergâ€™s Terms of Use](https://www.gutenberg.org/policy/terms_of_use.html).

```
